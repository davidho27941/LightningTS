common:
  model_name: &model_name <model_name>
  model_version: &model_version <model_version>
  log_dir: &logdir <logdir>
  epochs: &epochs 100
  batch_size: &batch_size 128 

loggers: &loggers
  common: &common_logger_config
    name: *model_name
    version: *model_version
  CSVLogger:
    <<: *common_logger_config
    save_dir: &csv_logger_dir [*logdir, "csv_log"]
  TensorBoardLogger:
    <<: *common_logger_config
    save_dir: &tb_logger_dir [*logdir, "tensorboard"]
    log_graph: True

data:
  dataset: <dataset_name>
  directory: </path/to/directory>
  format: <format>
  date_range:
    fit:
      start: <YYYY-mm-dd>
      end: <YYYY-mm-dd>
    validate:
      start: <YYYY-mm-dd>
      end: <YYYY-mm-dd>
    test:
      start: <YYYY-mm-dd>
      end: <YYYY-mm-dd>
  features: &features
    - feature1
    - feature2
    - feature3
    - feature4
  targets: &targets
    - target1
  total_size: &total_size <n_feature+n_target>
  series_config:
    input_length: 128
    label_length: 96
    output_length: 24
  preprocessing:
    freq: 't'
    shuffle: True
    seed: 42
    normalize_method: "MinMax"
    normalize_param: *csv_logger_dir
    time_encoding: True

dataloader:
  num_worker: 4

model:
  model_name: *model_name
  model_version: *model_version
  model_architecture: "PatchTST"
  hparam:
    lr: 0.0001
    optimizer: "adam"
    loss_fn: "l1Loss"
    batch_size: *batch_size
    lr_scheduler:
      name: "OneCycleLR"
      config:
  structure:
    common:
      pct_start: 0.4
      n_heads: 16
      d_model: 128
      dropout: 0.2
      fcn_dim: 256
      fc_dropout: 0.2
      activation: "gelu"
      kernel_size: 25
      stride: 8
      output_size: *total_size
      head_dropout: 0
      itr: 1
      train_epochs: *epochs
    encoder:
      n_layers: 3
      input_size: *total_size
  output_attention: False    
    
trainer:
  max_epochs: *epochs
  fast_dev_run: False
  accelerator: 
    type: "gpu"
    n_device: 1
  enable_fp16: False
  loggers:
    <<: *loggers
  callbacks:
    EarlyStopping:
      moniter: "val_loss"
      patience: 20
      mode: "min"
      men_delta: 0.001
      verbose: True
    ModelCheckpoint:
      moniter: "val_loss"
      every_n_epochs: 1
      save_on_train_epoch_end: True
      save_top_k: 1
      filename: "{epoch:02d}-{val_loss:.2f}"
    LearningRateMoniter:
      logging_interval: "epoch"
      log_momentum: True
    RichModelSummary:
      max_depth: 4

